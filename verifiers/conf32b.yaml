# Container image for this training run
image: "ghcr.io/tcapelle/triton_eval:1906"

# GRPO Training Configuration
seed: 42

# model_name_or_path: "/model-checkpoints/qwen-32b-vf_ep2-v2/checkpoint-500"
model_name_or_path: "/model-checkpoints/qwen-32b-vf_ep1-v2/checkpoint-700/"
dataset_name: "tcapelle/bootstrap_oai_pt_think"
field_messages: "prompt"
max_turns: 4

wandb_entity: "grpo-cuda"
wandb_project: "verifiers"
wandb_name: "qwen-32b-vf_ep3-v2-4steps"

output_dir: "/model-checkpoints/"

bf16: True

per_device_train_batch_size: 1
gradient_accumulation_steps: 12
num_generations: 8
num_train_epochs: 1
max_prompt_length: 16000 # 32768  # Remove length limit
max_completion_length: 16000 # 38912
beta: 0.0
temperature: 0.8
async_generation_timeout: 600
num_iterations: 1

learning_rate: 1e-3
max_grad_norm: 1e-7
lr_scheduler_type: "constant_with_warmup"
warmup_steps: 10
gradient_checkpointing: True

save_strategy: "steps"
save_steps: 100
save_only_model: True

logging_steps: 1
log_on_each_node: False
log_completions: False
save_total_limit: 3

# specify the vllm address and port
vllm_server_host: "cw-verifiers-vllm-service"
vllm_server_port: 8000

###############################################################
## rewards server
triton_server_url: "http://cw-verifiers-rewards-service-grpo:9347"
triton_run_endpoint: "/run_triton"
triton_benchmark: True
triton_benchmark_runs: 10

###############################################################
## vllm config
vllm:
  host: "0.0.0.0"
  gpu_memory_utilization: 0.95  # Ratio of GPU memory to reserve (0-1)
  max_model_len: 32000         # Maximum model sequence length
  tensor_parallel_size: 8